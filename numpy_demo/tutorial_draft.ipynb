{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Tutorial!\n",
    "\n",
    "First I'll introduce the theory behind neural nets. then we will implement one from scratch in numpy, (which is installed on the uni computers) - just type this code into your text editor of choice. I'll also show you how to define a neural net in googles DL library Tensorflow(which is not installed on the uni computers) and train it to clasify handwritten digits. \n",
    "\n",
    "You will understand things better if you're familiar with calculus and linear algebra, but the only thing you really need to know is basic programming. Don't worry if you don't understand the equations.\n",
    "\n",
    "## Numpy/linear algebra crash course\n",
    "\n",
    "(You should be able to run this all in python 2.7.8 on the uni computers.)\n",
    "\n",
    "Vectors and matrices are the language of neural networks. For our purposes, a vector is a list of numbers and a matrix is a 2d grid of numbers. Both can be defined as instances of numpy's ndarray class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  9]\n",
      " [10 20 30]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "my_vector = np.asarray([1,2,3])\n",
    "my_matrix = np.asarray([[1,2,3],[10,10,10]])\n",
    "print(my_matrix*my_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting an ndarray through a function will apply it elementwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   4   9]\n",
      " [100 100 100]]\n"
     ]
    }
   ],
   "source": [
    "print((my_matrix**2))\n",
    "print((my_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What is a neural network? \n",
    "For our data-sciencey purposes, it's best to think of a neural network as a *function approximator* or a *statistical model*. Surprisingly enough they are made up of a network of neurons. What is a neuron?\n",
    "\n",
    "_WARNING: huge oversimplification that will make neuroscientists cringe._\n",
    "\n",
    "![title](img/bioneuron.png)\n",
    "\n",
    "This is what a neuron in your brain looks like. On the right are the *axons*, on the left are the *dendrites*, which recieve signals from the axons of other neurons. The dendrites are connected to the axons with *synapses*. If the neuron has enough voltage across, it will \"spike\" and send a signal through its axon to neighbouring neurons. Some synapses are *excitory* in that if a signal goes through them it will increase the voltage across the next neuron, making it more likely to spike. Others are *inhibitory*\n",
    "and do the opposite. We learn by changing the strengths of synapses(well, kinda), and that is also usually how artificial neural networks learn.\n",
    "\n",
    "![title](img/perceptron.png)\n",
    "\n",
    "This is what a the simplest possible artificial neuron looks like. This neuron is connected to two other input neurons named \\\\(x_1 \\\\) and \\\\( x_2\\\\) with \"synapses\" \\\\(w_1\\\\) and \\\\(w_2\\\\). All of these symbols are just numbers(real/float).\n",
    "To get the neurons output signal \\\\(h\\\\), just sum the input neurons up, weighted by their \"synapses\" then put them through a nonlinear function \\\\( f\\\\):\n",
    "$$ h = f(x_1 w_1 + x_2  w_2)$$\n",
    "\n",
    "\n",
    "\\\\(f\\\\) can be anything that maps a real number to a real number, but for ML you want something nonlinear and smooth. For this neuron, \\\\(f\\\\) is the *sigmoid function*:\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}} $$\n",
    "Sigmoid squashes its output into [0,1], so it's closer to \"fully firing\" the more positive it's input, and closer to \"not firing\" the more negative it's input.\n",
    "![img](img/sigmoid.png)\n",
    "\n",
    "If you like to think in terms of graph theory, neurons are nodes and \n",
    "If you have a stats background you might have noticed that this looks similar a logistic regression on two variables. That's because it is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, these artificial neurons are only loosely inspired by biological neurons. That's ok, our goal is to have a good model, not simulate a brain.\n",
    "\n",
    "There are many exciting ways to arange these neurons into a network, but we will focus on one of the easier, more useful topologies called a \"two layer perceptron\", which looks like this:\n",
    "\n",
    "![mlp](img/mlp.png)\n",
    "\n",
    "Neurons are arranged in layers, with the first hidden layer of neurons connected to a vector(think list of numbers) of input data, \\\\(x\\\\), sometimes referred to as an \"input layer\". Every neuron in a given layer is connected to every neuron in the previous layer.\n",
    "\n",
    "$$net = \\sum_{i=0}^{N}x_i w_i = \\vec{x} \\cdot \\vec{w}$$\n",
    "\n",
    "Where \\\\(\\vec{x}\\\\) is a vector of previous layer's neuron activations and \\\\(\\vec{w} \\\\) is a vector of the weights(synapses) for every \\\\(x \\in \\vec{x} \\\\).\n",
    "\n",
    "Look back at the diagram again. Each of these 4 hidden units will have a vector of 3 weights for each of the inputs. We can arrange them as a 3x4 *matrix* of row vectors, which we call \\\\(W_1\\\\). Then we can multiply this matrix with \\\\(\\vec{x}\\\\) and apply our nonlinearity \\\\(f\\\\) to get a vector of neuron activations:\n",
    "$$\\vec{h} = f( \\vec{x} \\cdot W_1 )$$\n",
    "..actually, in practice we add a unique learnable \"bias\" \\\\(b\\\\) to every neurons weighted sum, which has the effect of shifting the nonlinearity left or right:\n",
    "$$\\vec{h} = f( \\vec{x} \\cdot W_1 + \\vec{b}_1 )$$\n",
    "We pretty much do the same thing to get the output for the second hidden layer, but with a different weight matrix \\\\(W_2\\\\):\n",
    "$$\\vec{h_2} = f( \\vec{h_1} \\cdot W_2 + \\vec{b}_2 )$$\n",
    "\n",
    "So if we want to get an output for a given data vector x, we can just plug it into these equations. Here it is in numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2ae6c0c740eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhidden_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "hidden_1 = sigmoid(x.dot(W1) + b_1)\n",
    "output = hidden1.dot(W2) + b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Well that's all very nice, but we need it to be able to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N,D = 300,2 #  number of examples, dimension of examples \n",
    "X = np.random.uniform(size=(N,D),low=0,high=20)\n",
    "y = [X[i,0] * X[i,1] for i in range(N)]\n",
    "\n",
    "class TwoLayerPerceptron:\n",
    "    \"\"\"Simple implementation of the most basic neural net\"\"\"\n",
    "    def __init__(self,X,H,Y):\n",
    "        N,D = X.shape\n",
    "        N,O = y.shape\n",
    "        # initialize the weights, or \"connections between neurons\" to random values.\n",
    "        self.W1 = np.random.normal(size=(D,H))\n",
    "        self.b1 = np.zeros(size=(H,))\n",
    "        self.W2 = np.random.normal(size=(H,O))\n",
    "        self.b2 = np.random.normal(size=(O,))\n",
    "        \n",
    "    def forward_pass(X):\n",
    "        \"\"\"Get the outputs for batch X, and a cache of hidden states for backprop\"\"\"\n",
    "        hidden_inputs = X.dot(W1) + b #matrix multiply\n",
    "        hidden_activations = relu(hidden_inputs)\n",
    "        output = hidden_activations.dot(W2) + b\n",
    "        cache = [X, hidden_inputs, ]\n",
    "        return cache\n",
    "    \n",
    "    def backwards_pass(self,cache):\n",
    "        \"\"\" \"\"\"\n",
    "        [X,hidden_inputs, hidden_activations, output] = cache\n",
    "        #//TODO: backwards pass\n",
    "        return d_W1, d_W2, d_b1, d_b2\n",
    "    \n",
    "    def subtract_gradients(self,gradients,lr=0.001):\n",
    "        [d_W1, d_W2, d_b1, d_b2] = gradients\n",
    "        self.W1 -= lr * d_W1\n",
    "        self.W2 -= lr * d_W2\n",
    "        self.b1 -= lr * d_b1\n",
    "        self.b2 -= lr * d_b2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_activations = relu(np.dot(X,W1) + b1)\n",
    "output = np.dot(hidden_activations,W2)+b2\n",
    "errors = 0.5 * (output - y)**2\n",
    "d_h1 = np.dot((output - y),W2.T)\n",
    "d_b1  = np.sum(d_h1,axis=1)\n",
    "d_a1 = sigmoid()\n",
    "d_W2 = np.dot(hidden_Activations, errors)\n",
    "d_W1 = np.dot(d_h1, W1.T)\n",
    "W_2 += d_W2\n",
    "b1 += db1\n",
    "W_1 += d_W1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(Math(r'h_1 = \\sigma(X \\cdot W_1 + b)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
